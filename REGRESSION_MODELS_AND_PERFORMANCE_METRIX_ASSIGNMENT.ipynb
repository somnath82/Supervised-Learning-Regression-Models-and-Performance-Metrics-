{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "            REGRESSION MODELS AND PERFORMANCE METRIX\n",
        "                      --- ASSIGNMENT---\n",
        "  "
      ],
      "metadata": {
        "id": "pNvJHPy5vBlX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression(SLR)? Explain its purpose .\n",
        "\n",
        "ANS:"
      ],
      "metadata": {
        "id": "DGd0YwqjxL9u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f688e24"
      },
      "source": [
        "## Simple Linear Regression (SLR)\n",
        "\n",
        "Simple Linear Regression is a statistical method used to model the relationship between two continuous variables: one independent variable (predictor) and one dependent variable (response). It assumes a linear relationship between these variables, meaning that the change in the dependent variable is proportional to the change in the independent variable.\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "The primary purpose of SLR is to understand and quantify the relationship between two variables. It allows us to:\n",
        "\n",
        "* **Predict:** Estimate the value of the dependent variable based on the value of the independent variable.\n",
        "* **Understand the relationship:** Determine the strength and direction of the linear association between the variables (e.g., positive or negative correlation).\n",
        "* **Identify outliers:** Detect data points that deviate significantly from the predicted linear relationship.\n",
        "* **Test hypotheses:** Assess whether there is a statistically significant relationship between the variables.\n",
        "\n",
        "In essence, SLR provides a simple yet powerful tool for analyzing and modeling the linear dependence between two variables, enabling predictions and insights into their relationship."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the key assumptions of simple linear regression ?\n",
        "\n",
        "ANS:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "az0tnXQg8wEK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea4f5b3b"
      },
      "source": [
        "## Key Assumptions of Simple Linear Regression\n",
        "\n",
        "Simple Linear Regression relies on several key assumptions for its results to be valid and reliable. These assumptions relate to the characteristics of the data and the relationship between the variables. Violations of these assumptions can lead to inaccurate predictions and misleading interpretations. The key assumptions are:\n",
        "\n",
        "1.  **Linearity:** The relationship between the independent variable (X) and the dependent variable (Y) is linear. This means that the change in Y for a unit change in X is constant across the range of X. You can visually check this assumption with a scatter plot of X against Y.\n",
        "\n",
        "2.  **Independence of Errors:** The errors (residuals) are independent of each other. This means that the error for one observation does not influence the error for another observation. Violations of this assumption often occur with time series data where consecutive observations may be correlated.\n",
        "\n",
        "3.  **Homoscedasticity (Constant Variance of Errors):** The variance of the errors is constant across all levels of the independent variable. In other words, the spread of the residuals is roughly the same for all values of X. Heteroscedasticity (non-constant variance) can be visualized by plotting the residuals against the predicted values.\n",
        "\n",
        "4.  **Normality of Errors:** The errors (residuals) are normally distributed. This assumption is particularly important for hypothesis testing and constructing confidence intervals. You can check this assumption using a histogram of the residuals or a normal probability plot.\n",
        "\n",
        "5.  **No Multicollinearity (for multiple regression, but relevant in concept for SLR):** While this is primarily a concern in multiple linear regression (where you have more than one independent variable), the underlying principle is that the independent variable should not be perfectly correlated with another variable in the model (in SLR, there's only one independent variable, so perfect multicollinearity isn't an issue, but understanding that independent variables should provide unique information is relevant).\n",
        "\n",
        "It's important to check these assumptions before interpreting the results of a Simple Linear Regression analysis. If these assumptions are violated, you may need to transform the data, use a different type of regression model, or employ robust regression techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write the mathematical equation for a simple linear regression model and explain each term.\n",
        "\n",
        "ANS:"
      ],
      "metadata": {
        "id": "OBU5Ut1u9swG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "188f7b56"
      },
      "source": [
        "## Mathematical Equation for Simple Linear Regression\n",
        "\n",
        "The mathematical equation for a simple linear regression model is typically represented as:\n",
        "\n",
        "$ Y = \\beta_0 + \\beta_1 X + \\epsilon $\n",
        "\n",
        "Let's break down each term:\n",
        "\n",
        "*   **$Y$**: This is the **dependent variable** (also known as the response variable or outcome variable). It's the variable we are trying to predict or explain.\n",
        "\n",
        "*   **$X$**: This is the **independent variable** (also known as the predictor variable or explanatory variable). This is the variable we are using to predict or explain $Y$.\n",
        "\n",
        "*   **$\\beta_0$ (Beta-zero)**: This is the **y-intercept**. It represents the expected value of $Y$ when $X$ is equal to 0. In some contexts, it might not have a meaningful interpretation if $X=0$ is outside the range of your data.\n",
        "\n",
        "*   **$\\beta_1$ (Beta-one)**: This is the **slope** of the regression line. It represents the change in the expected value of $Y$ for a one-unit increase in $X$. It quantifies the strength and direction of the linear relationship between $X$ and $Y$.\n",
        "\n",
        "*   **$\\epsilon$ (Epsilon)**: This is the **error term** (also known as the residual). It represents the part of $Y$ that cannot be explained by the linear relationship with $X$. It includes all other factors that influence $Y$ but are not included in the model. The assumptions of simple linear regression primarily concern the properties of this error term."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Provide a real-world example where simple linear regression can be applied.\n",
        "\n",
        "ANS:\n"
      ],
      "metadata": {
        "id": "OdwG1ffc_Y7d"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74321ed8"
      },
      "source": [
        "## Real-World Example of Simple Linear Regression\n",
        "\n",
        "A common real-world example of Simple Linear Regression is examining the relationship between **study time** (independent variable, X) and **exam score** (dependent variable, Y) for a group of students.\n",
        "\n",
        "In this scenario:\n",
        "\n",
        "*   **Independent Variable (X):** The number of hours a student spends studying for an exam.\n",
        "*   **Dependent Variable (Y):** The score the student achieves on the exam.\n",
        "\n",
        "By collecting data on several students (their study time and their exam scores), we can use Simple Linear Regression to:\n",
        "\n",
        "1.  **Determine if there is a linear relationship:** Does more study time generally lead to a higher exam score?\n",
        "2.  **Quantify the relationship:** How much does the exam score increase for each additional hour of studying? This would be represented by the slope ($\\beta_1$) of the regression line.\n",
        "3.  **Predict:** Based on the model, we could predict the expected exam score for a student who studies a certain number of hours.\n",
        "\n",
        "This simple example illustrates how SLR can be used to understand, quantify, and predict the relationship between two continuous variables in a practical setting. Other examples could include:\n",
        "\n",
        "*   The relationship between the number of years of experience and salary.\n",
        "*   The relationship between advertising expenditure and sales revenue.\n",
        "*   The relationship between the temperature and ice cream sales."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is the method of least squares in linear regression ?\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "3YRAqmTSADeJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "519a83b4"
      },
      "source": [
        "## The Method of Least Squares in Linear Regression\n",
        "\n",
        "The **method of least squares** is a fundamental technique used in linear regression to find the best-fitting straight line through a set of data points. The \"best-fitting\" line is defined as the one that minimizes the sum of the squared differences between the observed values of the dependent variable and the values predicted by the line.\n",
        "\n",
        "Here's a breakdown of the concept:\n",
        "\n",
        "1.  **The Goal:** In linear regression, we want to find a line that represents the linear relationship between the independent variable (X) and the dependent variable (Y) as closely as possible. This line is often called the regression line.\n",
        "\n",
        "2.  **Errors/Residuals:** For each data point, there is a difference between the actual observed value of Y and the value of Y that the regression line predicts for the corresponding X value. This difference is called the **error** or **residual** ($\\epsilon$).\n",
        "\n",
        "3.  **Minimizing the Errors:** We want to find a line that makes these errors as small as possible. However, simply summing the errors wouldn't work because positive and negative errors would cancel each other out.\n",
        "\n",
        "4.  **Squaring the Errors:** To overcome this, we square each error term. Squaring ensures that all the differences are positive and gives more weight to larger errors.\n",
        "\n",
        "5.  **Sum of Squared Errors (SSE):** We then sum up all the squared errors for all the data points. This sum is called the **Sum of Squared Errors (SSE)** or **Residual Sum of Squares (RSS)**.\n",
        "\n",
        "6.  **Finding the Best Line:** The method of least squares involves finding the values for the intercept ($\\beta_0$) and the slope ($\\beta_1$) of the regression line that result in the smallest possible SSE. Calculus is typically used to derive formulas for $\\beta_0$ and $\\beta_1$ that minimize the SSE.\n",
        "\n",
        "In essence, the method of least squares provides a mathematical way to determine the unique linear equation that best summarizes the relationship between two variables by minimizing the overall prediction errors in a squared sense. This method is widely used because it has desirable statistical properties and is computationally straightforward."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is logistic regression? How does it differ from Linear Regression ?\n",
        "\n",
        "ANS:"
      ],
      "metadata": {
        "id": "1EjjPHrEAkQc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ed50919"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "**Logistic Regression** is a statistical model used for **binary classification**. This means it's used to predict the probability that an instance belongs to a particular class (usually represented as 0 or 1). Unlike linear regression, which predicts a continuous outcome, logistic regression predicts a probability, which is then used to classify the instance into one of two categories.\n",
        "\n",
        "The core of logistic regression is the **logistic function** (also called the sigmoid function), which squashes any real-valued input into a value between 0 and 1. This output can be interpreted as a probability.\n",
        "\n",
        "The equation for logistic regression can be written in terms of the log-odds (logit) as:\n",
        "\n",
        "$ \\text{logit}(P(Y=1|X)) = \\beta_0 + \\beta_1 X $\n",
        "\n",
        "Where:\n",
        "*   $P(Y=1|X)$ is the probability that the dependent variable Y is 1 given the independent variable X.\n",
        "*   $\\beta_0$ is the intercept.\n",
        "*   $\\beta_1$ is the coefficient for the independent variable X.\n",
        "\n",
        "This can be transformed using the logistic function to get the probability:\n",
        "\n",
        "$ P(Y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}} $\n",
        "\n",
        "**How it differs from Linear Regression:**\n",
        "\n",
        "The key differences between Logistic Regression and Linear Regression lie in their purpose, the type of dependent variable they handle, and the function they use:\n",
        "\n",
        "1.  **Purpose:**\n",
        "    *   **Linear Regression:** Used for **regression tasks**, predicting a continuous numerical value.\n",
        "    *   **Logistic Regression:** Used for **classification tasks**, predicting the probability of an instance belonging to a specific category (binary or multi-class, though simple logistic regression is binary).\n",
        "\n",
        "2.  **Dependent Variable:**\n",
        "    *   **Linear Regression:** The dependent variable is **continuous** (e.g., salary, temperature).\n",
        "    *   **Logistic Regression:** The dependent variable is **categorical** (e.g., yes/no, spam/not spam, 0/1).\n",
        "\n",
        "3.  **Output:**\n",
        "    *   **Linear Regression:** The output is a **continuous value** that can range from negative infinity to positive infinity.\n",
        "    *   **Logistic Regression:** The output is a **probability** between 0 and 1, which is then used to assign a class label.\n",
        "\n",
        "4.  **Function:**\n",
        "    *   **Linear Regression:** Uses a **linear function** ($Y = \\beta_0 + \\beta_1 X$) to model the relationship.\n",
        "    *   **Logistic Regression:** Uses the **logistic (sigmoid) function** to transform the linear combination of inputs into a probability.\n",
        "\n",
        "In summary, while both are linear models in their parameters, their fundamental difference lies in the nature of the dependent variable and the transformation applied to the linear combination of independent variables to produce the output. Linear regression models a linear relationship for continuous outcomes, while logistic regression models the probability of a categorical outcome using a sigmoid function."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Name and briefly describe three common evaluation metrics for regression models.\n",
        "\n",
        "ANS:"
      ],
      "metadata": {
        "id": "G1g0DQlWBFOc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bac86b5"
      },
      "source": [
        "## Common Evaluation Metrics for Regression Models\n",
        "\n",
        "Here are three common evaluation metrics used to assess the performance of regression models:\n",
        "\n",
        "1.  **Mean Absolute Error (MAE):**\n",
        "    *   **Description:** MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. It's the average of the absolute differences between the predicted values and the actual values.\n",
        "    *   **Formula:** $ MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $\n",
        "        *   $y_i$: actual value\n",
        "        *   $\\hat{y}_i$: predicted value\n",
        "        *   $n$: number of observations\n",
        "    *   **Interpretation:** A lower MAE indicates a better fit. MAE is less sensitive to outliers compared to Mean Squared Error.\n",
        "\n",
        "2.  **Mean Squared Error (MSE):**\n",
        "    *   **Description:** MSE measures the average of the squared errors. It calculates the average of the squared differences between the predicted and actual values.\n",
        "    *   **Formula:** $ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $\n",
        "        *   $y_i$: actual value\n",
        "        *   $\\hat{y}_i$: predicted value\n",
        "        *   $n$: number of observations\n",
        "    *   **Interpretation:** A lower MSE indicates a better fit. Squaring the errors gives more weight to larger errors, making MSE sensitive to outliers.\n",
        "\n",
        "3.  **Root Mean Squared Error (RMSE):**\n",
        "    *   **Description:** RMSE is the square root of the Mean Squared Error. It represents the standard deviation of the residuals.\n",
        "    *   **Formula:** $ RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $\n",
        "        *   $y_i$: actual value\n",
        "        *   $\\hat{y}_i$: predicted value\n",
        "        *   $n$: number of observations\n",
        "    *   **Interpretation:** RMSE is in the same units as the dependent variable, which makes it more interpretable than MSE. A lower RMSE indicates a better fit. Like MSE, RMSE is also sensitive to outliers.\n",
        "\n",
        "These metrics provide different perspectives on the model's performance and are often used in combination to get a comprehensive understanding of how well the regression model is performing."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the purpose of the R-squard metric in regression analysis?\n",
        "\n",
        "ANS:"
      ],
      "metadata": {
        "id": "Np7K3Wq8BqhY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbaaff8b"
      },
      "source": [
        "## Purpose of the R-squared Metric\n",
        "\n",
        "**R-squared ($R^2$)**, also known as the coefficient of determination, is a statistical metric used in regression analysis to assess the **goodness of fit** of a regression model. Its primary purpose is to explain the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n",
        "\n",
        "Here's a breakdown of its purpose and interpretation:\n",
        "\n",
        "*   **Quantifying Explained Variance:** R-squared tells us how well the independent variable(s) in the model explain the variation in the dependent variable. A higher R-squared value indicates that a larger proportion of the variance in the dependent variable is accounted for by the model.\n",
        "*   **Range of Values:** R-squared values range from 0 to 1 (or 0% to 100%).\n",
        "    *   An R-squared of **0** means that the model explains none of the variability in the dependent variable around its mean. In this case, the model is no better than simply predicting the mean of the dependent variable for every observation.\n",
        "    *   An R-squared of **1** means that the model explains all the variability in the dependent variable around its mean. This would indicate a perfect fit, where all the data points fall exactly on the regression line.\n",
        "*   **Interpretation:** While a higher R-squared generally indicates a better fit, it's important to interpret it in context. A high R-squared doesn't necessarily mean the model is good or that the independent variable(s) are the only factors influencing the dependent variable. It only indicates the proportion of variance explained by the variables included in the model.\n",
        "*   **Limitations:** R-squared can be misleading, especially in multiple linear regression. Adding more independent variables to a model, even if they are not truly related to the dependent variable, will often increase the R-squared value. This is why adjusted R-squared is often preferred in multiple regression, as it accounts for the number of predictors in the model.\n",
        "\n",
        "In summary, R-squared provides a measure of how much of the variation in the dependent variable is explained by the independent variable(s) in the regression model. It's a useful metric for evaluating the overall fit of the model, but it should be considered along with other evaluation metrics and domain knowledge."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write Python code to fit a simple linear regression model using scikit-learn and print the slope and intercept.\n",
        "\n",
        "ANS:"
      ],
      "metadata": {
        "id": "hGFPb6ZXCN5n"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aeb8a15",
        "outputId": "8a18b8f9-f2a8-4daf-8354-85fcb0f58c28"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Independent variable\n",
        "y = np.array([2, 4, 5, 4, 5])  # Dependent variable\n",
        "\n",
        "# Create a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model to the data\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print the intercept and slope\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "print(\"Slope:\", model.coef_[0])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: 2.2\n",
            "Slope: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do you interpret the coefficients in a simple liear regression model."
      ],
      "metadata": {
        "id": "n2g-95isDBg6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96df2837"
      },
      "source": [
        "## Interpreting Coefficients in Simple Linear Regression\n",
        "\n",
        "In a simple linear regression model, the equation is $ Y = \\beta_0 + \\beta_1 X + \\epsilon $. The coefficients we are interested in interpreting are $\\beta_0$ (the intercept) and $\\beta_1$ (the slope).\n",
        "\n",
        "*   **Intercept ($\\beta_0$)**:\n",
        "    *   The intercept represents the predicted value of the dependent variable (Y) when the independent variable (X) is equal to 0.\n",
        "    *   **Interpretation:** If X can realistically be 0 within the context of your data, the intercept tells you the expected baseline value of Y. However, if X=0 is outside the range of your data or doesn't make practical sense, the intercept might not have a meaningful real-world interpretation on its own. It's still necessary for defining the regression line, but its individual value might not be directly interpretable.\n",
        "\n",
        "*   **Slope ($\\beta_1$)**:\n",
        "    *   The slope represents the change in the predicted value of the dependent variable (Y) for a one-unit increase in the independent variable (X).\n",
        "    *   **Interpretation:** This is often the most important coefficient to interpret. It quantifies the strength and direction of the linear relationship between X and Y.\n",
        "        *   A **positive slope** ($\\beta_1 > 0$) indicates that as X increases, Y tends to increase.\n",
        "        *   A **negative slope** ($\\beta_1 < 0$) indicates that as X increases, Y tends to decrease.\n",
        "        *   The **magnitude** of the slope tells you how much Y is expected to change for each one-unit change in X.\n",
        "\n",
        "**Example (using the code from the previous step):**\n",
        "\n",
        "In the previous code, we got an intercept of **2.2** and a slope of **0.6**.\n",
        "\n",
        "*   **Intercept (2.2):** This means that when the independent variable (X) is 0, the predicted value of the dependent variable (y) is 2.2. In the context of the sample data (where X represents [1, 2, 3, 4, 5]), X=0 is outside the data range, so the intercept's direct interpretation might not be meaningful in a real-world scenario.\n",
        "\n",
        "*   **Slope (0.6):** This means that for every one-unit increase in the independent variable (X), the predicted value of the dependent variable (y) increases by 0.6. So, if X increases from 1 to 2, we predict y to increase by 0.6.\n",
        "\n",
        "It's important to remember that these interpretations are based on the assumption that the linear model is appropriate for the data and that the key assumptions of linear regression are reasonably met."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}